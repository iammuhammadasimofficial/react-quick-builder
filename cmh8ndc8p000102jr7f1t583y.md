---
title: "Assistant API ‚Üí Responses API: A Complete, Practical Migration Guide (with Next.js & Node examples)"
seoTitle: "A Complete Migration Guide - OpenAI Assistant API to Responses API"
seoDescription: "Learn how to migrate from OpenAI‚Äôs legacy Assistant API (threads, messages, runs) to the new Responses API using modern streaming, typed inputs, MCP etc."
datePublished: Sat Oct 25 2025 19:00:00 GMT+0000 (Coordinated Universal Time)
cuid: cmh8ndc8p000102jr7f1t583y
slug: migration-guide-assistant-api-to-responses-api
cover: https://cdn.hashnode.com/res/hashnode/image/upload/v1761539863432/d0b74ff3-e6e3-4bdf-93e6-0531a6737be4.png
ogImage: https://cdn.hashnode.com/res/hashnode/image/upload/v1761539984015/cbeb03d2-89d2-4fdb-a154-99bd0ae3c8bf.png
tags: open-ai, muhammad-asim, responses-api, assistant-api-to-responses-api, openai-responses-api-migration-guide

---

 **TL;DR**: The Responses API gives you simpler request/response semantics, richer streaming events (including function calls), first‚Äëclass tool use, and persistent conversations. This guide shows exactly how to migrate a production app‚Äîhow to handle streaming function calls, switch Threads‚ÜíConversations, transform attachments, wire connectors via MCP, support built‚Äëin tools (web search, image generation), and add Whisper for audio. And how to create a robust streaming hook pattern you?

---

## Why Responses API (and what changed)

* **Mental model**: With Responses, you send **input items** and receive **output items**. The API also surfaces granular streaming events for text deltas and tool calls, so you can execute functions as arguments stream in and then resume the model. ([OpenAI Platform][1])
* **State**: You can manage multi‚Äëturn state with the new **Conversations API**, or by chaining with `previous_response_id` when you don‚Äôt need long‚Äëlived conversations. ([OpenAI Platform][2])
* **Tools & connectors**: Tools (web search, file search, image generation, code interpreter) and **connectors** (MCP) are first‚Äëclass in Responses. The platform handles approvals by default; you control this when appropriate. ([OpenAI Platform][3])
* **Streaming**: You can stream text, function‚Äëcall arguments, and completions; you can also stream while routing tool calls and then continue the same turn. ([OpenAI Platform][4])

---

## What maps to what (at a glance)

| Old (Assistants)         | New (Responses)                                            |
| ------------------------ | ---------------------------------------------------------- |
|  Threads & Messages       |  **Conversations** + `responses.create()` with input items  |
|  Runs + polling           |  **Streaming** responses and **streamed tool calls**        |
|  Function calling         |  **Function tools** (same JSON Schema, better streaming UX) |
|  File/image attachments   |  **Input items**: `input_text`, `input_file`, `input_image` |
|  Vector store/file search |  **File Search** tool                                       |
|  Third‚Äëparty APIs         |  **Connectors** (MCP servers)                               |

**Docs**: Responses overview & migration guidance. ([OpenAI Platform][5])

---

## Core migration strategy

1. **Replace Runs + polling ‚Üí Responses streaming **(SSE).
2. **Custom streaming hook** that consumes **Response events**: we accumulate function args, run handlers (parallel), then **resume** the model with tool outputs.
3. **Conversations**: automatically migrate old IDs to new `conv_*` and save the mapping.
4. **Attachments**: simple ‚Äúitems‚Äù format for text, files, images.
5. **Tools**: we enable built‚Äëins (web search, image generation, file search) plus **connectors** (via MCP) based on user selection; and we register **custom function tools**.
6. **Audio**: transcription with Whisper / gpt‚Äë4o‚Äëtranscribe (file or realtime). ([OpenAI Platform][6])

---

## 1) Core API route: from polling to streaming

### ‚ñ∂ Next.js (Route Handler: `app/api/responses/route.ts`)

```ts
// app/api/responses/route.ts
import OpenAI from "openai";

export const runtime = "edge"; // works well with streaming

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

export async function POST(req: Request) {
  const body = await req.json();

  // Build tools array dynamically (built-ins + connectors + custom functions)
  const tools: any[] = [
    { type: "web_search" },                          // built-in web search
    { type: "image_generation", model: "gpt-image-1" }, // built-in image gen
    // Custom functions are added below (examples later)
  ];

  // Add connectors (MCP) if the caller selected them and you have tokens
  if (body.selectedConnectors?.includes("gmail") && body.gmailAccessToken) {
    tools.push({
      type: "mcp",
      server_label: "google_gmail",
      connector_id: "connector_gmail",
      authorization: body.gmailAccessToken,
      // choose an approval mode that fits your UX & risk model
      require_approval: "never",
    });
  }

  // Add your custom function tools (schemas shown later)
  tools.push(...getCustomFunctionToolDefinitions());

  // Conversations: continue or create new one
  const conversation =
    typeof body.conversationId === "string" && body.conversationId.startsWith("conv_")
      ? { id: body.conversationId }
      : undefined;

  // Stream the response (SSE under the hood)
  const stream = await client.responses.stream({
    model: body.model ?? "gpt-4o",
    conversation,                       // optional: bind to a conversation
    previous_response_id: body.prevId,  // or chain lightweight state
    input: body.input,                  // items: input_text/input_image/input_file
    tools,
  });

  // OPTIONAL: hook into events server-side to drive function calls, logs, etc.
  wireResponseStream(stream, {
    onTextDelta: (t) => {/* forward to client logger if desired */},
    onFunctionCall: (call) => {/* accumulate args per call_id */},
    onFunctionCallArgsDelta: ({ call_id, delta }) => {/* buffer */},
    onFunctionCallArgsDone: async (complete) => {
      // When arguments for a call_id are complete, run your function
      await handleFunctionCall(complete);
    },
    onError: (e) => console.error(e),
  });

  // Return as a web stream to the browser
  return new Response(await stream.toReadableStream());
}

// --- streaming hook (general-purpose) ---
type StreamHandlers = {
  onTextDelta?: (text: string) => void;
  onFunctionCall?: (e: any) => void;
  onFunctionCallArgsDelta?: (e: any) => void;
  onFunctionCallArgsDone?: (e: { call_id: string; name: string; arguments: string }) => void;
  onError?: (e: unknown) => void;
};

function wireResponseStream(stream: any, h: StreamHandlers) {
  // Text deltas
  stream.on("response.output_text.delta", (e: any) => {
    h.onTextDelta?.(e.delta);
  });
  // Function call created (tells you which function & call_id)
  stream.on("response.output_item.added", (e: any) => {
    if (e.item?.type === "function_call") h.onFunctionCall?.(e.item);
  });
  // Function call arguments streaming (chunk deltas)
  stream.on("response.function_call_arguments.delta", (e: any) => {
    h.onFunctionCallArgsDelta?.(e);
  });
  // Function call arguments completed
  stream.on("response.function_call_arguments.done", (e: any) => {
    h.onFunctionCallArgsDone?.(e);
  });
  stream.on("error", (e: any) => h.onError?.(e));
}

// Provide your JSON Schemas separately
function getCustomFunctionToolDefinitions() {
  return [
    {
      type: "function",
      name: "get_weather",
      description: "Get weather by city name",
      parameters: {
        type: "object",
        properties: { city: { type: "string" }, unit: { type: "string", enum: ["C", "F"] } },
        required: ["city"],
      },
    },
    {
      type: "function",
      name: "lookup_order",
      description: "Look up an order by id and return status",
      parameters: {
        type: "object",
        properties: { orderId: { type: "string" } },
        required: ["orderId"],
      },
    },
  ];
}
```

**Why this works**: Responses streaming emits rich events, e.g., `response.output_text.delta` for text and `response.function_call_arguments.*` for tool calls‚Äîyou can read argument chunks, execute your handler when `done` fires, and then **continue the same turn** with the tool outputs. ([OpenAI Platform][4])

> **Docs**: Responses streaming, function‚Äëcall argument events. ([OpenAI Platform][4])

---

### ‚ñ∂ Node.js (Express)

```ts
// server/index.ts
import express from "express";
import OpenAI from "openai";

const app = express();
app.use(express.json());

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

app.post("/api/responses", async (req, res) => {
  const { model, input, prevId, conversationId, selectedConnectors, gmailAccessToken } = req.body;

  const tools: any[] = [
    { type: "web_search" },
    { type: "image_generation", model: "gpt-image-1" },
    // Add custom function tool definitions (same as Next.js)
    ...getCustomFunctionToolDefinitions(),
  ];

  if (selectedConnectors?.includes("gmail") && gmailAccessToken) {
    tools.push({
      type: "mcp",
      server_label: "google_gmail",
      connector_id: "connector_gmail",
      authorization: gmailAccessToken,
      require_approval: "never",
    });
  }

  const stream = await client.responses.stream({
    model: model ?? "gpt-4o",
    input,
    tools,
    conversation: conversationId?.startsWith("conv_") ? { id: conversationId } : undefined,
    previous_response_id: prevId,
  });

  // Pipe SSE directly to client
  res.setHeader("Content-Type", "text/event-stream");
  res.setHeader("Cache-Control", "no-cache");
  res.flushHeaders();

  // Forward the SSE bytes
  const reader = await stream.toReadableStream();
  const readerStream = reader.getReader();
  try {
    while (true) {
      const { done, value } = await readerStream.read();
      if (done) break;
      res.write(value);
    }
  } finally {
    res.end();
  }
});

app.listen(3000, () => console.log("Listening on http://localhost:3000"));

function getCustomFunctionToolDefinitions() {
  return [
    {
      type: "function",
      name: "get_weather",
      description: "Get weather by city name",
      parameters: {
        type: "object",
        properties: { city: { type: "string" }, unit: { type: "string", enum: ["C", "F"] } },
        required: ["city"],
      },
    },
    {
      type: "function",
      name: "lookup_order",
      description: "Look up an order by id and return status",
      parameters: {
        type: "object",
        properties: { orderId: { type: "string" } },
        required: ["orderId"],
      },
    },
  ];
}
```

---

## 2) Handling **function calls** (including **parallel** custom tools)

**Flow**:

1. Model decides to call function(s).
2. You receive **streaming arg deltas**, then a `done` with the full JSON args.
3. Execute your function(s)‚Äî**in parallel** when safe.
4. **Resume** the same turn by sending tool outputs back (correlated to the `call_id`).
5. The model reads your outputs and finishes the answer.

> In Responses, tool calls and tool outputs are separate item types linked by a `call_id`. You can chain via `previous_response_id` or by passing tool outputs as new input items. ([OpenAI Platform][5])

### ‚ñ∂ Next.js ‚Äî parallel tool execution + resume

```ts
// A lightweight in-memory accumulator for call args
const pending: Record<string, { name: string; json: any }> = {};

async function handleFunctionCall(e: { call_id: string; name: string; arguments: string }) {
  const args = JSON.parse(e.arguments || "{}");
  pending[e.call_id] = { name: e.name, json: args };
}

// After multiple calls reach *.done you can dispatch parallel work and resume:
async function completeToolPhaseAndResume(params: {
  // id from the streaming response (the turn to resume)
  previous_response_id: string;
  client: OpenAI;
}) {
  // Fan out parallel executions
  const entries = Object.entries(pending);
  const results = await Promise.all(
    entries.map(async ([call_id, { name, json }]) => {
      let output: unknown;
      if (name === "get_weather") {
        output = await weatherService(json.city, json.unit);
      } else if (name === "lookup_order") {
        output = await orderService(json.orderId);
      } else {
        output = { error: `Unknown tool ${name}` };
      }
      return { call_id, output };
    }),
  );

  // Clear local buffer for next turn
  for (const key of Object.keys(pending)) delete pending[key];

  // Send tool results back to the model and let it finish the turn.
  // Pattern A: pass tool outputs as inputs correlated by call_id.
  // (This mirrors the "items" model described in the migration guide.)
  const resumed = await params.client.responses.create({
    previous_response_id: params.previous_response_id,
    input: results.map((r) => ({
      role: "tool",
      content: [
        {
          type: "tool_result",
          call_id: r.call_id,        // correlate to the original call
          output_text: JSON.stringify(r.output),
        },
      ],
    })),
  });

  return resumed;
}
```

> **Docs**: Streaming events (incl. `response.function_call_arguments.delta/.done`) and the Responses migration guide explaining items & call correlation. ([OpenAI Platform][7])

> **Parallelism**: Function calling supports **parallel** tool calls‚Äîinvoke multiple services concurrently and post all results before resuming. ([OpenAI Platform][8])

### ‚ñ∂ Node.js (Express)

Implementation is the same idea as the Next.js sample above‚Äîaccumulate arg chunks per `call_id`, run handlers in parallel, then `responses.create({ previous_response_id, input: [tool_result‚Ä¶] })` to finalize the turn.

---

## 3) **Web search** and **image generation** as built‚Äëin tools

Enable the built‚Äëins by adding them to `tools`:

```ts
const tools = [
  { type: "web_search" },                       // lets the model search the web
  { type: "image_generation", model: "gpt-image-1" },  // generate/edit images
];
```

* The **web search** tool lets models fetch real‚Äëtime context inside a single call. It‚Äôs configured like any tool and the model decides when to use it. ([OpenAI Platform][9])
* The **image generation** tool uses `gpt-image-1` from the Responses API. ([OpenAI Platform][10])

---

## 4) **Connectors (MCP)** ‚Äî e.g., Gmail

Connectors let a model call third‚Äëparty services (Gmail, Dropbox, etc.) via MCP. Add a connector tool only if the user opted in and you hold a valid token.

### ‚ñ∂ Next.js

```ts
// When building tools[]
if (selectedConnectors.includes("gmail") && gmailAccessToken) {
  tools.push({
    type: "mcp",
    server_label: "google_gmail",
    connector_id: "connector_gmail",
    authorization: gmailAccessToken,
    // choose approval mode for your UX:
    // "auto" (default approvals), "never", or "always" depending on risk profile
    require_approval: "never",
  });
}
```

* By default, the platform prompts for approvals before any data is sent to a connector/MCP server; configure approval flow based on your trust model. ([OpenAI Platform][11])
* MCP is the protocol; **connectors** are first‚Äëparty integrations exposed via the same tool surface as remote MCP servers. ([OpenAI Platform][12])

---

## 5) Attachments and inputs (images/files/text)

Under Responses, you send items instead of Message arrays. Typical patterns:

```ts
// Images only
input: [{
  role: "user",
  content: [
    { type: "input_text",  text: "Compare these images" },
    { type: "input_image", image_url: "file:file-abc123" },
    { type: "input_image", image_url: "file:file-def456" },
  ],
}]

// Files only
input: [{
  role: "user",
  content: [
    { type: "input_text", text: "What is in this PDF?" },
    { type: "input_file", file_id: "file-xyz789" },
  ],
}]

// Mixed
input: [{
  role: "user",
  content: [
    { type: "input_text",  text: "Analyze this data" },
    { type: "input_image", image_url: "file:file-abc123" },
    { type: "input_file",  file_id: "file-xyz789" },
  ],
}]
```

* PDFs, images and other files can be sent via file IDs you uploaded to `/v1/files`. ([OpenAI Platform][13])
* Vision & image docs cover `input_image` usage; Responses supports both analysis and generation paths. ([OpenAI Platform][10])

---

## 6) Threads ‚Üí **Conversations** (automatic migration)

We migrated Assistant ‚Äúthreads‚Äù to **Conversations**:

### ‚ñ∂ Next.js

```ts
import OpenAI from "openai";
const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

export async function migrateThreadIdIfNeeded(oldId?: string) {
  if (!oldId || oldId.startsWith("conv_")) return oldId;

  const conv = await client.conversations.create({
    metadata: {
      migratedFrom: oldId,
      migrationDate: new Date().toISOString(),
    },
    // Optionally: seed with prior context as items
    // items: [...]
  });

  // Persist conv.id to your DB (map old->new)
  await saveConversationMapping(oldId, conv.id);
  return conv.id;
}
```

* **Conversations** are first‚Äëclass and integrate with Responses; you can also chain calls with `previous_response_id` for lighter‚Äëweight state. ([OpenAI Platform][2])

---

## 7) Audio transcription (Whisper / GPT‚Äë4o Transcribe)

### ‚ñ∂ Node.js (server)

```ts
import fs from "node:fs";
import OpenAI from "openai";

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

async function transcribe(filePath: string) {
  const file = fs.createReadStream(filePath);

  // Option A: Latest transcribe model
  const t1 = await client.audio.transcriptions.create({
    model: "gpt-4o-mini-transcribe",
    file,
    response_format: "json",
  });

  // Option B: Whisper (still supported)
  const t2 = await client.audio.transcriptions.create({
    model: "whisper-1",
    file,
  });

  return { t1, t2 };
}
```

* See Speech‚Äëto‚ÄëText & model docs (Whisper, GPT‚Äë4o‚Äëmini‚Äëtranscribe). ([OpenAI Platform][6])
* For **realtime** transcription, use the Realtime API with transcription sessions (WebSocket/WebRTC). ([OpenAI Platform][14])

---

## 8) Production‚Äëready streaming hook (what it listens for)

Our hook processes these events:

* `response.output_text.delta` ‚Üí stream tokens to UI
* `response.output_item.added` (when `item.type === "function_call"`) ‚Üí start buffering args
* `response.function_call_arguments.delta` ‚Üí append arg chunks
* `response.function_call_arguments.done` ‚Üí run your handler(s); if multiple calls, run in **parallel**
* (optional) `response.completed` / errors for cleanup

> These event names come from the Responses streaming interface. You‚Äôll find the function‚Äëcall argument delta/done events in the API reference. ([OpenAI Platform][4])

### Next.js client hook (brief & functional)

```ts
// app/hooks/useResponseStream.ts
"use client";
import { useEffect, useRef, useState } from "react";

type Call = { name: string; args: any };
export function useResponseStream(endpoint = "/api/responses") {
  const [text, setText] = useState("");
  const [calls, setCalls] = useState<Call[]>([]);
  const [status, setStatus] = useState<"idle"|"streaming"|"done"|"error">("idle");
  const esRef = useRef<EventSource|null>(null);
  const argBuffer = useRef<string>("");
  const currentFn = useRef<string>("");

  function start(prompt: string) {
    stop();
    setText(""); setCalls([]); setStatus("streaming");
    const url = endpoint + "?q=" + encodeURIComponent(prompt);
    const es = new EventSource(url); esRef.current = es;

    es.onmessage = (ev) => {
      try {
        const e = JSON.parse(ev.data);
        switch (e.type) {
          case "response.output_text.delta":
            setText(t => t + (e.delta ?? ""));
            break;
          case "response.output_item.added":
            if (e.item?.type === "function_call") { currentFn.current = e.item?.name ?? ""; argBuffer.current = ""; }
            break;
          case "response.function_call_arguments.delta":
            argBuffer.current += e.delta ?? "";
            break;
          case "response.function_call_arguments.done":
            setCalls(a => [...a, { name: currentFn.current, args: safe(argBuffer.current) }]);
            argBuffer.current = ""; currentFn.current = "";
            break;
          case "response.completed":
            setStatus("done"); stop();
            break;
        }
      } catch { /* ignore parse errors */ }
    };
    es.onerror = () => { setStatus("error"); stop(); };
  }

  function stop() { esRef.current?.close(); esRef.current = null; }
  useEffect(() => () => stop(), []);
  return { text, calls, status, start, stop };
}

function safe(s: string) { try { return JSON.parse(s || "{}"); } catch { return {}; } }
```

> Use `calls` to trigger your tool router in the UI (or forward to the server). Keep the hook UI-only: no secrets here.

---

## 9) Robust tool configuration (built‚Äëins, connectors, custom)

**Built‚Äëins** you‚Äôll likely enable:

* `web_search` (for real‚Äëtime info) ([OpenAI Platform][9])
* `image_generation` (with `gpt-image-1`) ([OpenAI Platform][10])
* `file_search` (for RAG over your uploaded files) ([OpenAI Platform][15])
* `code_interpreter` (sandboxed Python, data viz, light image ops) ([OpenAI Platform][16])

**Connectors (MCP)**:

* Add them conditionally, pass auth tokens, consider approval policy. ([OpenAI Platform][12])

**Custom functions**:

* Define via JSON Schema; keep inputs tight, validate outputs, and cap payload size (we cap response JSON around a few hundred KB to protect streaming/UI).
* For multi‚Äëtool steps, **parallelize** and submit results together before resuming. ([OpenAI Platform][8])

---

## 10) Error handling & ops check‚Äëlist

* **Streaming resilience**: timeouts, backoff, and ‚Äúcomplete tool phase then resume‚Äù patterns for flaky networks.
* **Approvals & privacy**: for MCP/connector calls, set approval behavior deliberately; avoid logging tokens or PII. ([OpenAI Platform][11])
* **Conversations**: persist `conv_*` IDs, and if you chain with `previous_response_id`, store the last `response.id` per user/thread. ([OpenAI Platform][17])
* **Attachments**: store OpenAI file IDs and any derived metadata; always generate `input_*` items server‚Äëside. ([OpenAI Platform][13])
* **Web search**: budget for per‚Äëcall search tool costs when the model elects to search. ([OpenAI Platform][18])

---

## End‚Äëto‚Äëend example: one call using web search, image gen, custom tools & Gmail connector (MCP)

### ‚ñ∂ Next.js

```ts
// app/api/chat/route.ts
import OpenAI from "openai";
export const runtime = "edge";

const client = new OpenAI({ apiKey: process.env.OPENAI_API_KEY! });

export async function POST(req: Request) {
  const body = await req.json();

  const tools: any[] = [
    { type: "web_search" },
    { type: "image_generation", model: "gpt-image-1" },
    ...customToolDefs(), // your function tools
  ];

  if (body.selectedConnectors?.includes("gmail") && body.gmailAccessToken) {
    tools.push({
      type: "mcp",
      server_label: "google_gmail",
      connector_id: "connector_gmail",
      authorization: body.gmailAccessToken,
      require_approval: "never",
    });
  }

  const stream = await client.responses.stream({
    model: "gpt-4o",
    conversation: body.conversationId ? { id: body.conversationId } : undefined,
    input: [
      {
        role: "user",
        content: [
          { type: "input_text", text: body.prompt },
          ...(body.imageFileIds || []).map((id: string) => ({ type: "input_image", image_url: `file:${id}` })),
          ...(body.fileIds || []).map((id: string) => ({ type: "input_file", file_id: id })),
        ],
      },
    ],
    tools,
  });

  // Optionally: intercept streaming events and handle custom tools (as earlier)
  // return streaming to client
  return new Response(await stream.toReadableStream());
}

function customToolDefs() {
  return [
    {
      type: "function",
      name: "fetch_price_quote",
      description: "Fetches a price quote by SKU and currency",
      parameters: {
        type: "object",
        properties: { sku: { type: "string" }, currency: { type: "string" } },
        required: ["sku", "currency"],
      },
    },
    {
      type: "function",
      name: "create_support_ticket",
      description: "Create a ticket with subject and body",
      parameters: {
        type: "object",
        properties: { subject: { type: "string" }, body: { type: "string" } },
        required: ["subject", "body"],
      },
    },
  ];
}
```

> Web search & image generation are built‚Äëins; Gmail is added via the MCP connector surface; custom functions use JSON Schema. ([OpenAI Platform][9])

### ‚ñ∂ Node.js (Express)

Same API surface as the Next.js example; the difference is streaming the `ReadableStream` into the HTTP response.

---

## GPT-5 + Markdown (important cookbook note)

> **üìò Note ‚Äî GPT-5 and Markdown formatting**
> The **OpenAI Cookbook** states: *‚ÄúBy default, GPT-5 in the API does not format its final answers in Markdown ‚Ä¶‚Äù* ‚Äî so if you want Markdown, **ask for it explicitly** in your system/instructions (e.g., ‚ÄúRespond in GitHub-flavored Markdown with headings and code blocks‚Äù). ([GitHub][11])

**Tip:** Set a **global system prompt** like:
‚ÄúAssistant must answer in **Markdown**, using `#` headings, fenced code blocks with language tags, and short paragraphs.‚Äù

---

## Migration checklist (copy/paste)

* [ ] Replace Assistants route(s) with a **Responses** route (streaming by default). ([OpenAI Platform][5])
* [ ] Add a **custom streaming hook** for `response.output_text.*` and `response.function_call_arguments.*` event handling. ([OpenAI Platform][4])
* [ ] Switch to **Conversations** (migrate old IDs ‚Üí `conv_*`) or use `previous_response_id` when long‚Äëlived state isn‚Äôt required. ([OpenAI Platform][2])
* [ ] Transform attachments to Responses **input items** (`input_text`, `input_image`, `input_file`). ([OpenAI Platform][13])
* [ ] Register built‚Äëins (**web search**, **image generation**, etc.) via `tools`. ([OpenAI Platform][9])
* [ ] Wire **connectors (MCP)** and approval policy; pass OAuth tokens securely. ([OpenAI Platform][12])
* [ ] Implement **custom functions** with strict JSON Schemas; support **parallel** tool calls; resume with tool results. ([OpenAI Platform][8])
* [ ] Add **audio transcription** (file or realtime) if needed. ([OpenAI Platform][6])
* [ ] Harden error handling: retries, timeouts, streaming cleanup, and safe fallbacks.

---

## Tips & gotchas

* Prefer **Conversations** when you want durable state and analytics; use `previous_response_id` to chain ad‚Äëhoc turns. Note `previous_response_id` only carries the last turn‚Äôs context unless you keep chaining. ([OpenAI Platform][17])
* Treat tool outputs as **data**, not UI‚Äîcap size, sanitize, and log carefully.
* For connectors, **opt‚Äëin** per user and avoid logging tokens; match your approval mode to the data risk. ([OpenAI Platform][11])
* For file inputs, upload to **Files API**, persist **file IDs**, and generate `input_file`/`input_image` on the server. ([OpenAI Platform][13])

---

## Appendix: Minimal input builders (client utilities)

> (No server call, so one tab)

### ‚ñ∂ Next.js / TypeScript

```ts
export function toInputItems(args: {
  text?: string;
  imageFileIds?: string[];
  fileIds?: string[];
}) {
  const content: any[] = [];
  if (args.text) content.push({ type: "input_text", text: args.text });
  (args.imageFileIds ?? []).forEach((id) => content.push({ type: "input_image", image_url: `file:${id}` }));
  (args.fileIds ?? []).forEach((id) => content.push({ type: "input_file", file_id: id }));
  return [{ role: "user", content }];
}
```

---

## FAQs

**1) Do I need Conversations or is `previous_response_id` enough?**
Use **Conversations** for durable threads/analytics. Use `previous_response_id` for lightweight chaining; it references prior context without storing a long-lived conversation server-side. ([Microsoft Learn][2])

**2) Where do I get event names like `response.output_text.delta` and `response.function_call_arguments.done`?**
They‚Äôre part of the Responses streaming interface and appear in SDK docs and references. Subscribe to them when using `responses.stream(‚Ä¶)`. ([OpenAI Platform][7])

**3) How do hosted tools differ from my function tools?**
Hosted tools (web search, file search, image gen, code interpreter) run on OpenAI infra and are enabled via `tools`. Function tools are **your** JSON-schema functions executed in your environment. ([OpenAI GitHub][10])

**4) How do I force Markdown output from GPT-5?**
Add an instruction: ‚ÄúAlways respond in Markdown (GFM) with headings and code fences.‚Äù By default, GPT-5 **does not** format answers as Markdown in the API. ([GitHub][11])

**5) Is Whisper deprecated if `gpt-4o-mini-transcribe` exists?**
No ‚Äî Whisper is still available; many teams prefer `gpt-4o-mini-transcribe` for accuracy/latency trade-offs. Evaluate both. ([OpenAI Platform][6])

---
## References

* **Streaming responses** & event types. ([OpenAI Platform][4])
* **Function calling** in Responses (incl. parallel). ([OpenAI Platform][19])
* **Conversations API** & conversation state. ([OpenAI Platform][2])
* **Migrate to Responses** (items, tools, chaining). ([OpenAI Platform][5])
* **Web search** & **tools** overview. ([OpenAI Platform][9])
* **Images & vision**; **image generation** tool. ([OpenAI Platform][10])
* **Files/PDF inputs**. ([OpenAI Platform][13])
* **Connectors/MCP** & approvals. ([OpenAI Platform][12])
* **Speech‚Äëto‚Äëtext** (Whisper & gpt‚Äë4o‚Äëmini‚Äëtranscribe). ([OpenAI Platform][6])

---

### Final word

Migrating to the Responses API simplifies your mental model and unlocks stronger agentic patterns: **streamed function calls**, **persistent conversations**, **built‚Äëins**, and **connectors**‚Äîall in one request. Use the patterns above (especially the streaming hook + parallel tool calls) and you‚Äôll have a robust, production‚Äëready setup.

[1]: https://platform.openai.com/docs/assistants/migration/what-has-changed "Assistants migration guide - OpenAI API"
[2]: https://platform.openai.com/docs/guides/conversation-state "Conversation state - OpenAI API"
[3]: https://platform.openai.com/docs/guides/tools "Using tools - OpenAI API"
[4]: https://platform.openai.com/docs/api-reference/responses-streaming "OpenAI Platform"
[5]: https://platform.openai.com/docs/guides/migrate-to-responses "Migrate to the Responses API - OpenAI API"
[6]: https://platform.openai.com/docs/guides/speech-to-text "Speech to text - OpenAI API"
[7]: https://platform.openai.com/docs/api-reference/responses-streaming/response/function_call_arguments/delta "OpenAI Platform"
[8]: https://platform.openai.com/docs/guides/function-calling/parallel-function-calling?api-mode=responses&utm_source=chatgpt.com "Function calling - OpenAI API"
[9]: https://platform.openai.com/docs/guides/tools-web-search "Web search - OpenAI API"
[10]: https://platform.openai.com/docs/guides/images-vision?api-mode=responses&utm_source=chatgpt.com "Images and vision - OpenAI API"
[11]: https://platform.openai.com/docs/guides/tools-connectors-mcp "Connectors and MCP servers - OpenAI API"
[12]: https://platform.openai.com/docs/guides/tools-connectors-mcp "OpenAI Platform"
[13]: https://platform.openai.com/docs/guides/pdf-files?api-mode=responses&utm_source=chatgpt.com "File inputs - OpenAI API"
[14]: https://platform.openai.com/docs/guides/realtime-transcription "Realtime transcription - OpenAI API"
[15]: https://platform.openai.com/docs/guides/tools-file-search "File search - OpenAI API"
[16]: https://platform.openai.com/docs/guides/tools-code-interpreter "Code Interpreter - OpenAI API"
[17]: https://platform.openai.com/docs/guides/conversation-state?api-mode=responses&utm_source=chatgpt.com "Conversation state - OpenAI API"
[18]: https://platform.openai.com/docs/models/gpt-4o-search-preview "Model - OpenAI API"
[19]: https://platform.openai.com/docs/guides/function-calling?api-mode=responses&lang=node.js&utm_source=chatgpt.com "Function calling - OpenAI API"
